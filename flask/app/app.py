# predictã«å¿…è¦ãªã‚‚ã®
# å­¦ç¿’æ¸ˆã¿model
# maxlenã®å€¤
# ä¿å­˜æ¸ˆã¿tokenizerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹

from typing import AsyncGenerator
from warnings import filterwarnings
from keras.preprocessing.sequence import pad_sequences
from keras import models
from keras.saving.save import load_model
import numpy as np
# import re
import pickle
# import json

from flask import Flask, render_template, request
from wtforms import Form, TextAreaField, SubmitField, validators, ValidationError
from janome.tokenizer import Tokenizer
# import numpy as np
# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_EVEN

import os
from flask import url_for
from flask_sqlalchemy import SQLAlchemy
from datetime import datetime
import pandas as pd
from janome.tokenizer import Tokenizer
from sklearn.model_selection import train_test_split
t = Tokenizer()

base_dir = os.path.dirname(__file__)

app = Flask(__name__)

app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' + os.path.join(base_dir, 'data.sqlite')
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

db = SQLAlchemy(app)

class Data(db.Model):
    __tablename__ = 'data'

    id = db.Column(db.Integer, primary_key=True)
    review = db.Column(db.Text)
    surprise = db.Column(db.Integer)
    sadness = db.Column(db.Integer)
    joy = db.Column(db.Integer)
    anger = db.Column(db.Integer)
    fear = db.Column(db.Integer)
    disgust = db.Column(db.Integer)
    trust = db.Column(db.Integer)
    anticipation = db.Column(db.Integer)
    timestamp = db.Column(db.DateTime)

    def __init__(self, review, surprise, sadness, joy, anger, fear, disgust, trust, anticipation):
        self.review = review
        self.surprise = surprise
        self.sadness = sadness
        self.joy = joy
        self.anger = anger
        self.fear = fear
        self.disgust = disgust
        self.trust = trust
        self.anticipation = anticipation
        self.timestamp = datetime.now()
        # formatted = datetime.datetime.strptime(test, "%Y-%m-%d %H:%M:%S.%f")

class TextForm(Form):
    Content = TextAreaField("åˆ†æã—ãŸã„æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ã­",
        [validators.InputRequired("ã“ã®é …ç›®ã¯å…¥åŠ›å¿…é ˆã§ã™"), validators.Length(max=140)])

    submit = SubmitField("è¨ºæ–­ã™ã‚‹")

def get_dataframe():
    conn = db.session.bind
    sql = "SELECT * FROM data"
    df = pd.read_sql(sql, conn)
    df = df.dropna()
    return df

def get_train_data(emotion):
    df = get_dataframe()
    df['review'] = df['review'].apply(wakati)
    X_train, X_test, y_train, y_test = train_test_split(df[['review']], df[['{}'.format(emotion)]], test_size=0.5, random_state=0)
    # X_train = df[['review']]
    # y_train = df[['joy']]
    loaded_tokenizer = load_text_tokenizer('tokenizer_ja')
    # loaded_tokenizer.fit_on_texts(X_train['review'])
    x_train = loaded_tokenizer.texts_to_sequences(X_train['review'])
    x_test = loaded_tokenizer.texts_to_sequences(X_test['review'])
    x_train_np = np.array(x_train)
    x_test_np = np.array(x_test)
    x_train1 = pad_sequences(x_train_np, maxlen=124)
    x_test1 = pad_sequences(x_test_np, maxlen=124)
    y_train_np = y_train.values
    y_test_np = y_test.values
    return x_train1, x_test1, y_train_np, y_test_np
emotion_dict = {
  'Avg. Readers_Surprise':'surprise',
  'Avg. Readers_Sadness':'sadness',
  'Avg. Readers_Joy':'joy',
  'Avg. Readers_Anger':'anger',
  'Avg. Readers_Fear':'fear',
  'Avg. Readers_Disgust':'disgust',
  'Avg. Readers_Trust':'trust',
  'Avg. Readers_Anticipation':'anticipation'
}
def update_model():
    model_list = ['Avg. Readers_Surprise', 'Avg. Readers_Sadness', 'Avg. Readers_Joy', 'Avg. Readers_Anger', 'Avg. Readers_Fear', 'Avg. Readers_Disgust', 'Avg. Readers_Trust', 'Avg. Readers_Anticipation']
    df = get_dataframe()
    timestamp = df.iloc[-1, -1]
    timestamp = datetime.strptime(timestamp, "%Y-%m-%d %H:%M:%S.%f")
    timestamp = timestamp.strftime('%Y%m%d-%H%M%S.%f')
    os.mkdir('model/{}'.format(timestamp))
    for model_name in model_list:
        x_train, x_test, y_train, y_test = get_train_data(emotion_dict[model_name])
        loaded_model = models.load_model(model_name + '.h5')
        loaded_model.fit(x_train, y_train, batch_size=1, epochs=3, validation_data=(x_test, y_test))
        loaded_model.save('model/{}/{}'.format(timestamp, model_name + '.h5'))

def get_latest_model():
    models_labels = os.listdir('model')
    models_created_at = []
    for model_label in models_labels:
        timestamp = datetime.strptime(model_label, "%Y%m%d-%H%M%S.%f")
        models_created_at.append(timestamp)
    latest_models = max(models_created_at)
    latest_models = latest_models.strftime('%Y%m%d-%H%M%S.%f')
    return latest_models

# from app import Data, db, get_train_data, text2vec, each_predict
# from sklearn.model_selection import train_test_split
# x_train, x_test, y_train, y_test = get_train_data()
# from keras import models
# model_name = 'Avg. Readers_Anger.h5'
# loaded_model = models.load_model(model_name)
# text = 'ä»Šæ—¥ã‚‚å…ƒæ°—ã«é ‘å¼µã‚‹ã‚ˆ'
# np_arr = text2vec(text)
# loaded_model.predict(np_arr)
# loaded_model.fit(x_train, y_train, batch_size=1, epochs=3, validation_data=(x_test, y_test))
# loaded_model.predict(np_arr)

def text2vec(text):

  wakati_ed = wakati(text)
  wakati_list = []
  wakati_list.append(wakati_ed)

  loaded_tokenizer = load_text_tokenizer('tokenizer_ja')
  tokenized = loaded_tokenizer.texts_to_sequences(wakati_list)

  np_arr = np.array(tokenized)
  np_arr = pad_sequences(np_arr, maxlen=124)

  return np_arr







@app.route('/', methods = ['GET', 'POST'])
def predicts():
    form = TextForm(request.form)
    if request.method == 'POST':
        if form.validate() == False:
            return render_template('index.html', form=form)
        else:
            Content = request.form["Content"]

            # ã“ã“ã§åˆ†æã™ã‚‹ã‚³ãƒ¼ãƒ‰
            # emotions = predict(Content)

            tmp = predict(Content)
            emotions = {}
            for key, value in tmp.items():
              emotions[key] = round(value[0][0]*100, 1)
            emotions = sorted(emotions.items(), key=lambda x:x[1], reverse=True)
            emotions_label_dic = {'Avg. Readers_Surprise':'é©šãğŸ˜²', 'Avg. Readers_Sadness':'æ‚²ã—ã¿ğŸ˜­', 'Avg. Readers_Joy':'å–œã³ğŸ˜„', 'Avg. Readers_Anger':'æ€’ã‚ŠğŸ’¢', 'Avg. Readers_Fear':'æã‚ŒğŸ˜¨', 'Avg. Readers_Disgust':'å«Œæ‚ªğŸ˜ ', 'Avg. Readers_Trust':'ä¿¡é ¼ğŸ¤', 'Avg. Readers_Anticipation':'æœŸå¾…ğŸ˜†'}

            return render_template('result.html', emotions=emotions, emotions_label_dic=emotions_label_dic, text=Content)
    elif request.method == 'GET':
        return render_template('index.html', form=form)

@app.route("/save_data", methods=['POST'])
def save_data():
  if request.method == 'POST':
    review = request.form.get('Content')
    surprise = request.form.get('surprise')
    sadness = request.form.get('sadness')
    joy = request.form.get('joy')
    anger = request.form.get('anger')
    fear = request.form.get('fear')
    disgust = request.form.get('disgust')
    trust = request.form.get('trust')
    anticipation = request.form.get('anticipation')

    data = Data(review, surprise, sadness, joy, anger, fear, disgust, trust, anticipation)
    db.session.add(data)
    db.session.commit()
    df = get_dataframe()
    # if df.iloc[-1, 0] % 10 == 0:
    update_model()
  return render_template('thanks.html')



# def remove_punct(text):
#     """
#     é¡”æ–‡å­—ï¼ˆemoticonsï¼‰ã®ã¿ã‚’æ®‹ã—ï½¤å¥èª­æ–‡å­—ï¼ˆpunctationï¼‰ã®å‰Šé™¤ã‚’è¡Œã†ï½¡
#     """
#     # ç›®ã®éƒ¨åˆ†: : or ; or =
#     # é¼»ã®éƒ¨åˆ†: -
#     # å£ã®éƒ¨åˆ†: ) or ( or D or P
#     pattern = re.compile(r"(?::|;|=)(?:-)?(?:\)|\(|D|P)")
#     # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å½“ã¦ã¯ã¾ã‚‹æ–‡å­—åˆ—ã‚’ã™ã¹ã¦æŠ½å‡ºã—ã¦ï½¤ãƒªã‚¹ãƒˆã«æ ¼ç´
#     # ã„ã£ãŸã‚“ãƒªã‚¹ãƒˆã«ä¿å­˜ã—ã¦ãŠã„ã¦ï½¤ã‚ã¨ã§ã™ã¹ã¦ã®è¨˜å·ã‚’å‰Šé™¤ã—ãŸtextã«ä»˜ã‘è¶³ã™
#     emoticons = pattern.findall(text)
    # # æ–‡é ­ãªã©ã«ã‚ã‚‹å¤§æ–‡å­—ã‚’å°æ–‡å­—ã«å¤‰æ›
    # lower = text.lower()
    # # [\W]+ã§è¨˜å·ã®ä¸¦ã³ã‚’æ•æ‰ã—ã¦ç©ºç™½ã²ã¨ã¤ã«ç½®ãæ›ãˆã‚‹
    # removed = re.sub(r"[\W]+", " ", lower)
    # # é¡”æ–‡å­—ã‚’åŠè§’ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§ã²ã¨ã¤ã®æ–‡å­—åˆ—ã«çµåˆ
    # emoticons = " ".join(emoticons)
    # # é¡”æ–‡å­—ã«å«ã¾ã‚Œã‚‹é¼»ã®éƒ¨åˆ†ã‚’å‰Šé™¤ã™ã‚‹
    # # é¼»ãŒã‚ã£ã¦ã‚‚ãªãã¦ã‚‚ï½¤ç›®ã¨å£ãŒåŒã˜ãªã‚‰åŒã˜é¡”æ–‡å­—ã¨ã—ã¦èªè­˜ã•ã‚Œã‚‹ã‚ˆã†ã«ãªã‚‹
    # emoticons = emoticons.replace("-","")
    # # lowerã¨emoticonsã‚’çµåˆ
    # # å˜èªã‚’åŒºåˆ‡ã‚‹ãŸã‚ï½¤é–“ã«ã¯åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’å…¥ã‚Œã¦ãŠã
    # connected = removed + ' ' + emoticons
    # return connected

def load_text_tokenizer(file_name):
  # loading
  with open(file_name+".pickle", 'rb') as handle:
      return pickle.load(handle)

# max_len = 124

# from janome.tokenizer import Tokenizer
# t = Tokenizer()
def wakati(text):
  tokens = t.tokenize(text)
  tmp_sentence = ''
  for token in tokens:
    tmp_sentence += token.surface + ' '
  return tmp_sentence

def load_text_tokenizer(file_name):
  # loading
  with open(file_name+".pickle", 'rb') as handle:
      return pickle.load(handle)

def each_predict(text, model_name, dir_name):

  wakati_ed = wakati(text)
  wakati_list = []
  wakati_list.append(wakati_ed)

  loaded_tokenizer = load_text_tokenizer('tokenizer_ja')
  tokenized = loaded_tokenizer.texts_to_sequences(wakati_list)

  np_arr = np.array(tokenized)
  np_arr = pad_sequences(np_arr, maxlen=124)
  path = 'model/{}/{}'.format(dir_name, model_name)
  loaded_model = models.load_model(path)
  result = loaded_model.predict(np_arr)
  # print(result)
  # if result >= 0.5:
  #   print(model_name)
  return result
# æ—¥æœ¬èª8ã¤ã®æ„Ÿæƒ…
def predict(text):
  dirname = get_latest_model()
  result_dic = {}
  model_list = ['Avg. Readers_Surprise', 'Avg. Readers_Sadness', 'Avg. Readers_Joy', 'Avg. Readers_Anger', 'Avg. Readers_Fear', 'Avg. Readers_Disgust', 'Avg. Readers_Trust', 'Avg. Readers_Anticipation']
  for model in model_list:
    result_dic[model] = each_predict(text, model + '.h5', dirname)
  return result_dic



# def predict(text):

#   removed = remove_punct(text)
#   removed_list = []
#   removed_list.append(removed)

#   loaded_tokenizer = load_text_tokenizer('tokenizer')
#   tokenized = loaded_tokenizer.texts_to_sequences(removed_list)

#   np_arr = np.array(tokenized)
#   max_len = 2505 #è‡ªå‹•åŒ–ã—ãŸã„
#   np_arr = pad_sequences(np_arr, maxlen=max_len)

#   loaded_model = models.load_model('ml14_lstm.h5')
#   result = loaded_model.predict(np_arr)
#   if result >= 0.5:
#     return 'positive'
#   else:
#     return 'negative'

if __name__ == "__main__":
    app.run(debug=True)

