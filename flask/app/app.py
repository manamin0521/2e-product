# predictã«å¿…è¦ãªã‚‚ã®
# å­¦ç¿’æ¸ˆã¿model
# maxlenã®å€¤
# ä¿å­˜æ¸ˆã¿tokenizerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹

from keras.preprocessing.sequence import pad_sequences
from keras import models
import numpy as np
# import re
import pickle
# import json

from flask import Flask, render_template, request
from werkzeug.utils import redirect
from wtforms import Form, TextAreaField, SubmitField, validators, ValidationError
from janome.tokenizer import Tokenizer
# import numpy as np
# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_EVEN

app = Flask(__name__)


class TextForm(Form):
    Content = TextAreaField("åˆ†æã—ãŸã„æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ä¸‹ã•ã„",
                            [validators.InputRequired("ã“ã®é …ç›®ã¯å…¥åŠ›å¿…é ˆã§ã™"), validators.Length(max=140)])

    submit = SubmitField("è¨ºæ–­ã™ã‚‹")


@app.route('/', methods=['GET'])
def top():
    return render_template('top.html')

@app.route('/about.html', methods=['GET'])
def about():
    return render_template('about.html')

@app.route('/index.html', methods=['GET', 'POST'])
def predicts():
    form = TextForm(request.form)
    if request.method == 'POST':
        if form.validate() == False:
            return render_template('index.html', form=form)
        else:
            Content = request.form["Content"]

            # ã“ã“ã§åˆ†æã™ã‚‹ã‚³ãƒ¼ãƒ‰
            # emotions = predict(Content)

            tmp = predict(Content)
            emotions = {}
            for key, value in tmp.items():
                emotions[key] = round(value[0][0]*100, 1)
            emotions = sorted(emotions.items(),
                              key=lambda x: x[1], reverse=True)
            emotions_label_dic = {'Avg. Readers_Surprise': 'é©šãğŸ˜²', 'Avg. Readers_Sadness': 'æ‚²ã—ã¿ğŸ˜­', 'Avg. Readers_Joy': 'å–œã³ğŸ˜„', 'Avg. Readers_Anger': 'æ€’ã‚ŠğŸ’¢',
                                  'Avg. Readers_Fear': 'æã‚ŒğŸ˜¨', 'Avg. Readers_Disgust': 'å«Œæ‚ªğŸ˜ ', 'Avg. Readers_Trust': 'ä¿¡é ¼ğŸ¤', 'Avg. Readers_Anticipation': 'æœŸå¾…ğŸ˜†'}

            return render_template('result.html', emotions=emotions, emotions_label_dic=emotions_label_dic)
    elif request.method == 'GET':
        return render_template('index.html', form=form)


# def remove_punct(text):
#     """
#     é¡”æ–‡å­—ï¼ˆemoticonsï¼‰ã®ã¿ã‚’æ®‹ã—ï½¤å¥èª­æ–‡å­—ï¼ˆpunctationï¼‰ã®å‰Šé™¤ã‚’è¡Œã†ï½¡
#     """
#     # ç›®ã®éƒ¨åˆ†: : or ; or =
#     # é¼»ã®éƒ¨åˆ†: -
#     # å£ã®éƒ¨åˆ†: ) or ( or D or P
#     pattern = re.compile(r"(?::|;|=)(?:-)?(?:\)|\(|D|P)")
#     # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å½“ã¦ã¯ã¾ã‚‹æ–‡å­—åˆ—ã‚’ã™ã¹ã¦æŠ½å‡ºã—ã¦ï½¤ãƒªã‚¹ãƒˆã«æ ¼ç´
#     # ã„ã£ãŸã‚“ãƒªã‚¹ãƒˆã«ä¿å­˜ã—ã¦ãŠã„ã¦ï½¤ã‚ã¨ã§ã™ã¹ã¦ã®è¨˜å·ã‚’å‰Šé™¤ã—ãŸtextã«ä»˜ã‘è¶³ã™
#     emoticons = pattern.findall(text)
    # # æ–‡é ­ãªã©ã«ã‚ã‚‹å¤§æ–‡å­—ã‚’å°æ–‡å­—ã«å¤‰æ›
    # lower = text.lower()
    # # [\W]+ã§è¨˜å·ã®ä¸¦ã³ã‚’æ•æ‰ã—ã¦ç©ºç™½ã²ã¨ã¤ã«ç½®ãæ›ãˆã‚‹
    # removed = re.sub(r"[\W]+", " ", lower)
    # # é¡”æ–‡å­—ã‚’åŠè§’ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§ã²ã¨ã¤ã®æ–‡å­—åˆ—ã«çµåˆ
    # emoticons = " ".join(emoticons)
    # # é¡”æ–‡å­—ã«å«ã¾ã‚Œã‚‹é¼»ã®éƒ¨åˆ†ã‚’å‰Šé™¤ã™ã‚‹
    # # é¼»ãŒã‚ã£ã¦ã‚‚ãªãã¦ã‚‚ï½¤ç›®ã¨å£ãŒåŒã˜ãªã‚‰åŒã˜é¡”æ–‡å­—ã¨ã—ã¦èªè­˜ã•ã‚Œã‚‹ã‚ˆã†ã«ãªã‚‹
    # emoticons = emoticons.replace("-","")
    # # lowerã¨emoticonsã‚’çµåˆ
    # # å˜èªã‚’åŒºåˆ‡ã‚‹ãŸã‚ï½¤é–“ã«ã¯åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’å…¥ã‚Œã¦ãŠã
    # connected = removed + ' ' + emoticons
    # return connected

def load_text_tokenizer(file_name):
    # loading
    with open(file_name+".pickle", 'rb') as handle:
        return pickle.load(handle)

# max_len = 124


t = Tokenizer()


def wakati(text):
    tokens = t.tokenize(text)
    tmp_sentence = ''
    for token in tokens:
        tmp_sentence += token.surface + ' '
    return tmp_sentence


def load_text_tokenizer(file_name):
    # loading
    with open(file_name+".pickle", 'rb') as handle:
        return pickle.load(handle)


def each_predict(text, model_name):

    wakati_ed = wakati(text)
    wakati_list = []
    wakati_list.append(wakati_ed)

    loaded_tokenizer = load_text_tokenizer('tokenizer_ja')
    tokenized = loaded_tokenizer.texts_to_sequences(wakati_list)

    np_arr = np.array(tokenized)
    np_arr = pad_sequences(np_arr, maxlen=124)

    loaded_model = models.load_model(model_name)
    result = loaded_model.predict(np_arr)
    # print(result)
    # if result >= 0.5:
    #   print(model_name)
    return result
# æ—¥æœ¬èª8ã¤ã®æ„Ÿæƒ…


def predict(text):
    result_dic = {}
    model_list = ['Avg. Readers_Surprise', 'Avg. Readers_Sadness', 'Avg. Readers_Joy', 'Avg. Readers_Anger',
                  'Avg. Readers_Fear', 'Avg. Readers_Disgust', 'Avg. Readers_Trust', 'Avg. Readers_Anticipation']
    for model in model_list:
        result_dic[model] = each_predict(text, model + '.h5')
    return result_dic


# def predict(text):

#   removed = remove_punct(text)
#   removed_list = []
#   removed_list.append(removed)

#   loaded_tokenizer = load_text_tokenizer('tokenizer')
#   tokenized = loaded_tokenizer.texts_to_sequences(removed_list)

#   np_arr = np.array(tokenized)
#   max_len = 2505 #è‡ªå‹•åŒ–ã—ãŸã„
#   np_arr = pad_sequences(np_arr, maxlen=max_len)

#   loaded_model = models.load_model('ml14_lstm.h5')
#   result = loaded_model.predict(np_arr)
#   if result >= 0.5:
#     return 'positive'
#   else:
#     return 'negative'
if __name__ == "__main__":
    app.run(debug=True)
